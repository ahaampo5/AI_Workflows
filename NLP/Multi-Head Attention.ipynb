{"cells":[{"cell_type":"code","execution_count":1,"id":"apparent-thread","metadata":{"id":"apparent-thread","executionInfo":{"status":"ok","timestamp":1641168748538,"user_tz":-540,"elapsed":6305,"user":{"displayName":"JCdata","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"18221923975897538727"}},"outputId":"b119cf22-e2ea-4bc7-e8b9-ba21b56666b5","colab":{"base_uri":"https://localhost:8080/"}},"outputs":[{"output_type":"stream","name":"stdout","text":["PyTorch version 1.10.0+cu111\n","Device cpu\n"]}],"source":["import numpy as np\n","\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","import torch.nn.functional as F\n","import matplotlib.pyplot as plt\n","%matplotlib inline\n","%config InlineBackend.figure_format='retina'\n","print('PyTorch version', torch.__version__)\n","device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n","print('Device', device)"]},{"cell_type":"code","execution_count":null,"id":"eligible-malpractice","metadata":{"id":"eligible-malpractice","outputId":"6bd4e60f-5d91-453d-f10e-d9c052fdfd0d"},"outputs":[{"name":"stdout","output_type":"stream","text":["SDPA: q torch.Size([3, 30, 128]), k torch.Size([3, 50, 128]), v torch.Size([3, 50, 256]) => out tensor([[[0.5766, 0.5519, 0.5141,  ..., 0.5295, 0.5224, 0.4360],\n","         [0.5804, 0.5519, 0.5139,  ..., 0.5262, 0.5243, 0.4370],\n","         [0.5761, 0.5604, 0.5065,  ..., 0.5249, 0.5131, 0.4304],\n","         ...,\n","         [0.5762, 0.5583, 0.5094,  ..., 0.5206, 0.5184, 0.4350],\n","         [0.5807, 0.5568, 0.5159,  ..., 0.5220, 0.5155, 0.4272],\n","         [0.5736, 0.5620, 0.5069,  ..., 0.5286, 0.5163, 0.4318]],\n","\n","        [[0.4617, 0.5079, 0.5258,  ..., 0.4722, 0.4513, 0.4566],\n","         [0.4561, 0.5017, 0.5272,  ..., 0.4734, 0.4552, 0.4551],\n","         [0.4561, 0.5011, 0.5229,  ..., 0.4817, 0.4483, 0.4593],\n","         ...,\n","         [0.4579, 0.5059, 0.5201,  ..., 0.4784, 0.4525, 0.4593],\n","         [0.4643, 0.5066, 0.5227,  ..., 0.4814, 0.4502, 0.4583],\n","         [0.4645, 0.5043, 0.5246,  ..., 0.4684, 0.4510, 0.4515]],\n","\n","        [[0.4631, 0.5184, 0.5659,  ..., 0.4898, 0.4570, 0.5155],\n","         [0.4721, 0.5099, 0.5541,  ..., 0.4969, 0.4601, 0.5191],\n","         [0.4653, 0.5222, 0.5598,  ..., 0.4931, 0.4577, 0.5133],\n","         ...,\n","         [0.4728, 0.5150, 0.5595,  ..., 0.4885, 0.4567, 0.5115],\n","         [0.4723, 0.5216, 0.5624,  ..., 0.4915, 0.4585, 0.5160],\n","         [0.4690, 0.5152, 0.5599,  ..., 0.4950, 0.4579, 0.5094]]]), attention tensor([[[0.0168, 0.0192, 0.0210,  ..., 0.0223, 0.0224, 0.0177],\n","         [0.0169, 0.0201, 0.0220,  ..., 0.0259, 0.0220, 0.0219],\n","         [0.0163, 0.0163, 0.0214,  ..., 0.0216, 0.0211, 0.0194],\n","         ...,\n","         [0.0159, 0.0197, 0.0193,  ..., 0.0250, 0.0222, 0.0205],\n","         [0.0149, 0.0206, 0.0232,  ..., 0.0225, 0.0228, 0.0236],\n","         [0.0174, 0.0232, 0.0230,  ..., 0.0208, 0.0184, 0.0190]],\n","\n","        [[0.0204, 0.0235, 0.0210,  ..., 0.0204, 0.0255, 0.0173],\n","         [0.0177, 0.0215, 0.0196,  ..., 0.0218, 0.0261, 0.0194],\n","         [0.0176, 0.0230, 0.0213,  ..., 0.0180, 0.0271, 0.0170],\n","         ...,\n","         [0.0178, 0.0232, 0.0221,  ..., 0.0181, 0.0223, 0.0196],\n","         [0.0177, 0.0231, 0.0208,  ..., 0.0197, 0.0264, 0.0174],\n","         [0.0189, 0.0212, 0.0201,  ..., 0.0223, 0.0243, 0.0153]],\n","\n","        [[0.0208, 0.0206, 0.0213,  ..., 0.0169, 0.0219, 0.0214],\n","         [0.0214, 0.0206, 0.0211,  ..., 0.0161, 0.0243, 0.0220],\n","         [0.0234, 0.0226, 0.0215,  ..., 0.0161, 0.0237, 0.0206],\n","         ...,\n","         [0.0262, 0.0217, 0.0191,  ..., 0.0197, 0.0208, 0.0179],\n","         [0.0221, 0.0225, 0.0200,  ..., 0.0159, 0.0214, 0.0206],\n","         [0.0244, 0.0214, 0.0186,  ..., 0.0149, 0.0218, 0.0193]]])\n","(Multi-Headed) SDPA: Q[3, 5, 30, 128] K[3, 5, 50, 128] V[3, 5, 50, 256] => out[3, 5, 30, 256] attention[3, 5, 30, 50]\n"]}],"source":["class ScaleDotProductAttention(nn.Module):\n","    def forward(self,q,k,v,mask=None):\n","        d_k = k.size(-1) # 확인해보자\n","        score = q @ k.transpose(-2,-1) / np.sqrt(d_k)\n","        if mask is not None:\n","            score = score.masked_fill(mask==0, -1e9)\n","        attention = F.softmax(score, dim=-1)\n","        out = attention.matmul(v)\n","        return out, attention\n","\n","SDPA = ScaleDotProductAttention()\n","n_batch, d_k, d_v = 3,128,256\n","n_q,n_k,n_v = 30,50,50\n","q = torch.rand(n_batch, n_q, d_k)\n","k = torch.rand(n_batch, n_k, d_k)\n","v = torch.rand(n_batch, n_v, d_v)\n","out,attention = SDPA.forward(q,k,v,mask=None)\n","\n","def sh(x): return str(x.shape)[11:-1]\n","print(f'SDPA: q {q.size()}, k {k.size()}, v {v.size()} => out {out}, attention {attention}')\n","\n","# It supports 'multi-headed' attention\n","n_batch,n_head,d_K,d_V = 3,5,128,256\n","n_Q,n_K,n_V = 30,50,50 # n_K and n_V should be the same\n","Q = torch.rand(n_batch,n_head,n_Q,d_K)\n","K = torch.rand(n_batch,n_head,n_K,d_K)\n","V = torch.rand(n_batch,n_head,n_V,d_V)\n","out,attention = SDPA.forward(Q,K,V,mask=None)\n","# out: [n_batch x n_head x n_Q x d_V]\n","# attention: [n_batch x n_head x n_Q x n_K] \n","def sh(x): return str(x.shape)[11:-1] \n","print (\"(Multi-Headed) SDPA: Q%s K%s V%s => out%s attention%s\"%\n","       (sh(Q),sh(K),sh(V),sh(out),sh(attention)))\n","    "]},{"cell_type":"code","execution_count":null,"id":"collaborative-looking","metadata":{"id":"collaborative-looking","outputId":"4f6e6cb7-c0e5-43c6-cd16-5b769fcc4aaf"},"outputs":[{"name":"stdout","output_type":"stream","text":["Input src:\t[128, 32, 200]  \t= [n_batch, n_src, d_feat]\n","\n","Q_feat:   \t[128, 32, 200]  \t= [n_batch, n_src, d_feat]\n","K_feat:   \t[128, 32, 200]  \t= [n_batch, n_src, d_feat]\n","V_feat:   \t[128, 32, 200]  \t= [n_batch, n_src, d_feat]\n","\n","Q_split:  \t[128, 5, 32, 40]  \t= [n_batch, n_head, n_src, d_head]\n","K_split:  \t[128, 5, 32, 40]  \t= [n_batch, n_head, n_src, d_head]\n","V_split:  \t[128, 5, 32, 40]  \t= [n_batch, n_head, n_src, d_head]\n","\n","scores:   \t[128, 5, 32, 32]  \t= [n_batch, n_head, n_src, n_src]\n","attention:\t[128, 5, 32, 32]  \t= [n_batch, n_head, n_src, n_src]\n","\n","x_raw:    \t[128, 5, 32, 40]  \t= [n_batch, n_head, n_src, d_head]\n","x_rsh1:   \t[128, 32, 5, 40]  \t= [n_batch, n_src, n_head, d_head]\n","x_rsh2:   \t[128, 32, 200]  \t= [n_batch, n_src, d_feat]\n","\n","Output x: \t[128, 32, 200]  \t= [n_batch, n_src, d_feat]\n"]}],"source":["class MultiHeadedAttention(nn.Module):\n","    def __init__(self,d_feat=128,n_head=5,actv=F.relu,USE_BIAS=True,dropout_p=0.1,device=None):\n","        \"\"\"\n","        :param d_feat: feature dimension\n","        :param n_head: number of heads\n","        :param actv: activation after each linear layer\n","        :param USE_BIAS: whether to use bias\n","        :param dropout_p: dropout rate\n","        :device: which device to use (e.g., cuda:0)\n","        \"\"\"\n","        super(MultiHeadedAttention,self).__init__()\n","        if (d_feat%n_head) != 0:\n","            raise ValueError(\"d_feat(%d) should be divisible by b_head(%d)\"%(d_feat,n_head)) \n","        self.d_feat = d_feat\n","        self.n_head = n_head\n","        self.d_head = self.d_feat // self.n_head\n","        self.actv = actv\n","        self.USE_BIAS = USE_BIAS\n","        self.dropout_p = dropout_p # prob. of zeroed\n","\n","        self.lin_Q = nn.Linear(self.d_feat,self.d_feat,self.USE_BIAS)\n","        self.lin_K = nn.Linear(self.d_feat,self.d_feat,self.USE_BIAS)\n","        self.lin_V = nn.Linear(self.d_feat,self.d_feat,self.USE_BIAS)\n","        self.lin_O = nn.Linear(self.d_feat,self.d_feat,self.USE_BIAS)\n","\n","        self.dropout = nn.Dropout(p=self.dropout_p)\n","    \n","    def forward(self,Q,K,V,mask=None):\n","        \"\"\"\n","        :param Q: [n_batch, n_Q, d_feat]\n","        :param K: [n_batch, n_K, d_feat]\n","        :param V: [n_batch, n_V, d_feat] <= n_K and n_V must be the same \n","        :param mask: \n","        \"\"\"\n","        n_batch = Q.shape[0]\n","        Q_feat = self.lin_Q(Q) \n","        K_feat = self.lin_K(K) \n","        V_feat = self.lin_V(V)\n","        # Q_feat: [n_batch, n_Q, d_feat]\n","        # K_feat: [n_batch, n_K, d_feat]\n","        # V_feat: [n_batch, n_V, d_feat]\n","\n","        # Multi-head split of Q, K, and V (d_feat = n_head*d_head)\n","        Q_split = Q_feat.view(n_batch, -1, self.n_head, self.d_head).permute(0, 2, 1, 3)\n","        K_split = K_feat.view(n_batch, -1, self.n_head, self.d_head).permute(0, 2, 1, 3)\n","        V_split = V_feat.view(n_batch, -1, self.n_head, self.d_head).permute(0, 2, 1, 3)\n","        # Q_split: [n_batch, n_head, n_Q, d_head]\n","        # K_split: [n_batch, n_head, n_K, d_head]\n","        # V_split: [n_batch, n_head, n_V, d_head]\n","\n","        # Multi-Headed Attention\n","        d_K = K.size()[-1] # key dimension\n","        scores = torch.matmul(Q_split,K_split.permute(0,1,3,2)) / np.sqrt(d_K)# FILL IN HERE\n","        if mask is not None:\n","            scores = scores.masked_fill(mask==0,-1e9)\n","        attention = torch.softmax(scores,dim=-1)\n","        x_raw = torch.matmul(self.dropout(attention),V_split) # dropout is NOT mentioned in the paper\n","        # attention: [n_batch, n_head, n_Q, n_K]\n","        # x_raw: [n_batch, n_head, n_Q, d_head]\n","\n","        # Reshape x\n","        x_rsh1 = x_raw.permute(0,2,1,3).contiguous()\n","        # x_rsh1: [n_batch, n_Q, n_head, d_head]\n","        x_rsh2 = x_rsh1.view(n_batch,-1,self.d_feat)\n","        # x_rsh2: [n_batch, n_Q, d_feat]\n","\n","        # Linear\n","        x = self.lin_O(x_rsh2)\n","        # x: [n_batch, n_Q, d_feat]\n","        out = {'Q_feat':Q_feat,'K_feat':K_feat,'V_feat':V_feat,\n","               'Q_split':Q_split,'K_split':K_split,'V_split':V_split,\n","               'scores':scores,'attention':attention,\n","               'x_raw':x_raw,'x_rsh1':x_rsh1,'x_rsh2':x_rsh2,'x':x}\n","        return out\n","\n","# Self-Attention Layer\n","n_batch = 128\n","n_src   = 32\n","d_feat  = 200\n","n_head  = 5\n","src = torch.rand(n_batch,n_src,d_feat)\n","self_attention = MultiHeadedAttention(\n","    d_feat=d_feat,n_head=n_head,actv=F.relu,USE_BIAS=True,dropout_p=0.1,device=device)\n","out = self_attention.forward(src,src,src,mask=None)\n","\n","Q_feat,K_feat,V_feat = out['Q_feat'],out['K_feat'],out['V_feat']\n","Q_split,K_split,V_split = out['Q_split'],out['K_split'],out['V_split']\n","scores,attention = out['scores'],out['attention']\n","x_raw,x_rsh1,x_rsh2,x = out['x_raw'],out['x_rsh1'],out['x_rsh2'],out['x']\n","\n","# Print out shapes\n","def sh(_x): return str(_x.shape)[11:-1] \n","print (\"Input src:\\t%s  \\t= [n_batch, n_src, d_feat]\"%(sh(src)))\n","print ()\n","print (\"Q_feat:   \\t%s  \\t= [n_batch, n_src, d_feat]\"%(sh(Q_feat)))\n","print (\"K_feat:   \\t%s  \\t= [n_batch, n_src, d_feat]\"%(sh(K_feat)))\n","print (\"V_feat:   \\t%s  \\t= [n_batch, n_src, d_feat]\"%(sh(V_feat)))\n","print ()\n","print (\"Q_split:  \\t%s  \\t= [n_batch, n_head, n_src, d_head]\"%(sh(Q_split)))\n","print (\"K_split:  \\t%s  \\t= [n_batch, n_head, n_src, d_head]\"%(sh(K_split)))\n","print (\"V_split:  \\t%s  \\t= [n_batch, n_head, n_src, d_head]\"%(sh(V_split)))\n","print ()\n","print (\"scores:   \\t%s  \\t= [n_batch, n_head, n_src, n_src]\"%(sh(scores)))\n","print (\"attention:\\t%s  \\t= [n_batch, n_head, n_src, n_src]\"%(sh(attention)))\n","print ()\n","print (\"x_raw:    \\t%s  \\t= [n_batch, n_head, n_src, d_head]\"%(sh(x_raw)))\n","print (\"x_rsh1:   \\t%s  \\t= [n_batch, n_src, n_head, d_head]\"%(sh(x_rsh1)))\n","print (\"x_rsh2:   \\t%s  \\t= [n_batch, n_src, d_feat]\"%(sh(x_rsh2)))\n","print ()\n","print (\"Output x: \\t%s  \\t= [n_batch, n_src, d_feat]\"%(sh(x)))\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"},"colab":{"name":"Multi-Head Attention.ipynb","provenance":[]}},"nbformat":4,"nbformat_minor":5}